{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HybridSN Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkVLbwolWSqwN0mucXoX5T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucalyptus/HybridSN-Pytorch/blob/master/HybridSN-ColabVersion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncvbvEXefOso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from scipy import io \n",
        "import torch.utils.data\n",
        "import scipy\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfJ-fvMXg4dm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import Module, Sequential, Conv2d, ReLU,AdaptiveMaxPool2d, AdaptiveAvgPool2d, \\\n",
        "    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NonXTSVhJbx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "ea1c2cef-966b-4e44-f82b-e4578f5e375a"
      },
      "source": [
        "!pip install -U spectral\n",
        "\n",
        "\n",
        "if not (os.path.isfile('/content/Indian_pines_corrected.mat')):\n",
        "  !wget http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n",
        "if not (os.path.isfile('/content/Indian_pines_gt.mat')):\n",
        "  !wget http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spectral\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/ff/f6e238a941ed55079526996fee315fbee5167aaa64de3e64980637ac8f38/spectral-0.21-py3-none-any.whl (187kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 13.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from spectral) (1.18.5)\n",
            "Installing collected packages: spectral\n",
            "Successfully installed spectral-0.21\n",
            "--2020-07-05 07:22:11--  http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n",
            "Resolving www.ehu.eus (www.ehu.eus)... 158.227.0.65, 2001:720:1410::65\n",
            "Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5953527 (5.7M) [text/plain]\n",
            "Saving to: ‘Indian_pines_corrected.mat’\n",
            "\n",
            "Indian_pines_correc 100%[===================>]   5.68M   349KB/s    in 19s     \n",
            "\n",
            "2020-07-05 07:22:31 (303 KB/s) - ‘Indian_pines_corrected.mat’ saved [5953527/5953527]\n",
            "\n",
            "--2020-07-05 07:22:33--  http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n",
            "Resolving www.ehu.eus (www.ehu.eus)... 158.227.0.65, 2001:720:1410::65\n",
            "Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1125 (1.1K) [text/plain]\n",
            "Saving to: ‘Indian_pines_gt.mat’\n",
            "\n",
            "Indian_pines_gt.mat 100%[===================>]   1.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-07-05 07:22:35 (98.9 MB/s) - ‘Indian_pines_gt.mat’ saved [1125/1125]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsHTfwTXhGUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.io as sio\n",
        "def loadData():\n",
        "    data = sio.loadmat('Indian_pines_corrected.mat')['indian_pines_corrected']\n",
        "    labels = sio.loadmat('Indian_pines_gt.mat')['indian_pines_gt']\n",
        "    \n",
        "    return data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP1VcW6yg7qU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padWithZeros(X, margin=2):\n",
        "\n",
        "    ## From: https://github.com/gokriznastic/HybridSN/blob/master/Hybrid-Spectral-Net.ipynb\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX\n",
        "\n",
        "def createImageCubes(X, y, windowSize=9, removeZeroLabels = True):\n",
        "\n",
        "     ## From: https://github.com/gokriznastic/HybridSN/blob/master/Hybrid-Spectral-Net.ipynb\n",
        "    margin = int((windowSize - 1) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]), dtype=np.uint8)\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]), dtype=np.uint8)\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k91V9cCfcBVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def applyPCA(X, numComponents=75):\n",
        "    newX = np.reshape(X, (-1, X.shape[2]))\n",
        "    pca = PCA(n_components=numComponents, whiten=True)\n",
        "    newX = pca.fit_transform(newX)\n",
        "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
        "    return newX, pca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVY_p30-PNmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "data_url,label_url = 'Indian_pines_corrected.mat','Indian_pines_gt.mat'\n",
        "X = np.array(scipy.io.loadmat('/content/'+data_url.split('/')[-1])[data_url.split('/')[-1].split('.')[0].lower()])\n",
        "y = np.array(scipy.io.loadmat('/content/'+label_url.split('/')[-1])[label_url.split('/')[-1].split('.')[0].lower()])\n",
        "X,_ = applyPCA(X,numComponents=30)\n",
        "X,y = createImageCubes(X,y, windowSize=25)\n",
        "X = np.expand_dims(X, axis=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.70, stratify=y)\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train).type(torch.FloatTensor),torch.from_numpy(y_train).type(torch.FloatTensor))\n",
        "train_loader = DataLoader(train_dataset, batch_size=16)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_val).type(torch.FloatTensor),torch.from_numpy(y_val).type(torch.FloatTensor))\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvxx12jBltia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "d3ad2264-289f-48a2-86c3-e15ff45aaad2"
      },
      "source": [
        "class HybridSN(nn.Module):\n",
        "    def __init__(self,band,classes):\n",
        "        super(HybridSN, self).__init__()\n",
        "        self.conv3d_1 = nn.Sequential(nn.Conv3d(1, 8, (3,3,3)), \n",
        "                        nn.ReLU())\n",
        "        \n",
        "        self.conv3d_2 = nn.Sequential(nn.Conv3d(8, 16, (3,3,7)),\n",
        "                        nn.ReLU())\n",
        "                        \n",
        "        self.conv3d_3 = nn.Sequential(nn.Conv3d( 16,32, (3,3,5)),\n",
        "                        nn.ReLU())\n",
        "        self.conv2d_1 = nn.Sequential(nn.Conv2d( 576,64, (3, 3)),\n",
        "                        nn.ReLU())\n",
        "        \n",
        "        self.dense1 =  nn.Linear(18496,256)\n",
        "        self.dense2 =  nn.Linear(256,128)\n",
        "        self.full = nn.Linear(128,classes)\n",
        "        self.drop = nn.Dropout(p=0.4)\n",
        "        self.soft = nn.Softmax(dim=-1)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv3d_1(x)\n",
        "        \n",
        "        x = self.conv3d_2(x)\n",
        "        \n",
        "        x = self.conv3d_3(x)\n",
        "        \n",
        "        batches,Q,H,W,C = x.size()\n",
        "        \n",
        "        x = x.view(batches,Q*C,H,W)\n",
        "        \n",
        "        x = self.conv2d_1(x)\n",
        "        \n",
        "        x = x.reshape(batches,-1)\n",
        "        \n",
        "        x = self.dense1(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.full(x)\n",
        "        \n",
        "\n",
        "        return self.soft(x)\n",
        "net = HybridSN(30,16).to(device)\n",
        "from torchsummary import summary\n",
        "print(summary(net,(1,25,25,30),batch_size=16))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv3d-1        [16, 8, 23, 23, 28]             224\n",
            "              ReLU-2        [16, 8, 23, 23, 28]               0\n",
            "            Conv3d-3       [16, 16, 21, 21, 22]           8,080\n",
            "              ReLU-4       [16, 16, 21, 21, 22]               0\n",
            "            Conv3d-5       [16, 32, 19, 19, 18]          23,072\n",
            "              ReLU-6       [16, 32, 19, 19, 18]               0\n",
            "            Conv2d-7           [16, 64, 17, 17]         331,840\n",
            "              ReLU-8           [16, 64, 17, 17]               0\n",
            "            Linear-9                  [16, 256]       4,735,232\n",
            "          Dropout-10                  [16, 256]               0\n",
            "           Linear-11                  [16, 128]          32,896\n",
            "          Dropout-12                  [16, 128]               0\n",
            "           Linear-13                   [16, 16]           2,064\n",
            "          Softmax-14                   [16, 16]               0\n",
            "================================================================\n",
            "Total params: 5,133,408\n",
            "Trainable params: 5,133,408\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.14\n",
            "Forward/backward pass size (MB): 122.21\n",
            "Params size (MB): 19.58\n",
            "Estimated Total Size (MB): 142.93\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjYpn1C_MX3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "ce_loss = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxCMoUOZTONi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c343f951-b520-4842-f7c0-7d0a0d85a13f"
      },
      "source": [
        "len(train_loader.dataset),len(test_loader.dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3074, 7175)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3AuPLYBPNoi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "12684019-1e8e-4288-c8e6-9a114655b479"
      },
      "source": [
        "def train(epoch):    \n",
        "    net.train()\n",
        "    for batch_idx, (data, targets ) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = net(data)\n",
        "        loss = ce_loss(output,targets.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),loss.item()))\n",
        "          \n",
        "\n",
        "\n",
        "for epoch in range(0, 100):\n",
        "    train(epoch)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/3074 (0%)]\tLoss: 2.768684\n",
            "Train Epoch: 0 [1600/3074 (52%)]\tLoss: 2.749597\n",
            "Train Epoch: 1 [0/3074 (0%)]\tLoss: 2.374597\n",
            "Train Epoch: 1 [1600/3074 (52%)]\tLoss: 2.749597\n",
            "Train Epoch: 2 [0/3074 (0%)]\tLoss: 2.312097\n",
            "Train Epoch: 2 [1600/3074 (52%)]\tLoss: 2.812097\n",
            "Train Epoch: 3 [0/3074 (0%)]\tLoss: 2.312097\n",
            "Train Epoch: 3 [1600/3074 (52%)]\tLoss: 2.749597\n",
            "Train Epoch: 4 [0/3074 (0%)]\tLoss: 2.312097\n",
            "Train Epoch: 4 [1600/3074 (52%)]\tLoss: 2.749597\n",
            "Train Epoch: 5 [0/3074 (0%)]\tLoss: 2.312096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-da6071f36df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-da6071f36df1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO6CkBJvT7ox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sampling(proportion, ground_truth):\n",
        "    train = {}\n",
        "    test = {}\n",
        "    labels_loc = {}\n",
        "    m = max(ground_truth)\n",
        "    for i in range(m):\n",
        "        indexes = [j for j, x in enumerate(ground_truth.ravel().tolist()) if x == i + 1]\n",
        "        np.random.shuffle(indexes)\n",
        "        labels_loc[i] = indexes\n",
        "        if proportion != 1:\n",
        "            nb_val = max(int((1 - proportion) * len(indexes)), 3)\n",
        "        else:\n",
        "            nb_val = 0\n",
        "        # print(i, nb_val, indexes[:nb_val])\n",
        "        # train[i] = indexes[:-nb_val]\n",
        "        # test[i] = indexes[-nb_val:]\n",
        "        train[i] = indexes[:nb_val]\n",
        "        test[i] = indexes[nb_val:]\n",
        "    train_indexes = []\n",
        "    test_indexes = []\n",
        "    for i in range(m):\n",
        "        train_indexes += train[i]\n",
        "        test_indexes += test[i]\n",
        "    np.random.shuffle(train_indexes)\n",
        "    np.random.shuffle(test_indexes)\n",
        "    return train_indexes, test_indexes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqYXINYdP5dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_test    = []\n",
        "with torch.no_grad():\n",
        "    for X, y in validation_loader:\n",
        "        X = X.to(device)\n",
        "        net.eval() \n",
        "        y_hat = net(X)\n",
        "        pred_test.extend(np.array(net(X).cpu().argmax(axis=1)))\n",
        "\n",
        "\n",
        "\n",
        "collections.Counter(pred_test)\n",
        "gt_test = gt[test_indices] - 1\n",
        "overall_acc    = metrics.accuracy_score(pred_test   , gt_test[:-VAL_SIZE])\n",
        "confusion_matrix    = metrics.confusion_matrix(pred_test , gt_test[:-VAL_SIZE])\n",
        "each_acc   , average_acc    = aa_and_each_accuracy(confusion_matrix)\n",
        "kappa = metrics.cohen_kappa_score(pred_test   , gt_test[:-VAL_SIZE])\n",
        "torch.save(net.state_dict(), \"./net/\" + str(round(overall_acc   , 3)) + '.pt')\n",
        "KAPPA.append(kappa)\n",
        "OA.append(overall_acc)\n",
        "AA.append(average_acc)\n",
        "ELEMENT_ACC[index_iter, :] = each_acc   \n",
        "\n",
        "print(\"--------\" + net.name + \" Training Finished-----------\")\n",
        "record.record_output(OA, AA, KAPPA, ELEMENT_ACC, TRAINING_TIME, TESTING_TIME,\n",
        "                     'records/' + method + '_' + Dataset + '_' +str(BAND)+ '_'  + str(VALIDATION_SPLIT)  + '.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrDzepOIXR6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}